{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📌 一、交叉熵的本质：衡量两个概率分布的“差距”\n",
    "\n",
    "在多分类任务中，模型的输出通常是一个**概率分布**，即通过 softmax 得到的各类别概率。\n",
    "\n",
    "而交叉熵就是衡量两个概率分布（真实标签和预测分布）之间“差异”的一种方式。\n",
    "\n",
    "---\n",
    "\n",
    "## 📘 二、公式详解\n",
    "\n",
    "### ✅ 情况1：用 one-hot 标签时（常见）\n",
    "\n",
    "设：\n",
    "- 真实标签（one-hot 编码）为 $ \\mathbf{y} = [y_1, y_2, ..., y_C] $，其中只有一个为 1，其他为 0；\n",
    "- 模型输出（softmax 概率）为 $ \\hat{\\mathbf{y}} = [\\hat{y}_1, \\hat{y}_2, ..., \\hat{y}_C] $；\n",
    "\n",
    "则交叉熵损失定义为：\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "因为只有 $ y_k = 1 $，其余为 0，所以上式简化为：\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\log(\\hat{y}_k)\n",
    "$$\n",
    "\n",
    "👉 也就是说：只惩罚模型在**真实类别上的概率预测是否足够高**。\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 情况2：使用 soft label 或 label smoothing\n",
    "\n",
    "如果真实标签不是 one-hot，而是一个概率分布，比如：\n",
    "\n",
    "$$\n",
    "y = [0.1, 0.8, 0.1]\n",
    "$$\n",
    "\n",
    "此时，交叉熵依然适用，表示两个概率分布之间的“差异”程度：\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 🔢 三、与 softmax 联合使用\n",
    "\n",
    "在多分类神经网络中，通常是：\n",
    "1. 最后一层输出 logits（未归一化分数）\n",
    "2. 对 logits 使用 softmax 转换成概率\n",
    "3. 与真实标签计算交叉熵损失\n",
    "\n",
    "例如，模型输出 logits 为：\n",
    "\n",
    "$$\n",
    "z = [1.2, 0.4, -0.5]\n",
    "$$\n",
    "\n",
    "softmax 得到：\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^{C} e^{z_j}}\n",
    "$$\n",
    "\n",
    "再带入交叉熵公式。\n",
    "\n",
    "💡 PyTorch 中的 `CrossEntropyLoss` 会**自动将 logits 做 softmax**，不用你自己手动加 softmax。\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 四、交叉熵的目标：最大化真实标签的预测概率\n",
    "\n",
    "交叉熵损失越小，说明预测越“接近”真实标签的概率分布。\n",
    "\n",
    "它本质上在做一件事：\n",
    "\n",
    "$$\n",
    "\\text{最小化交叉熵} \\quad = \\quad \\text{最大化预测中真实类别的概率}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均交叉熵损失： 0.3146948516368866\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "logits = torch.tensor([\n",
    "    [2.0, 0.5, 0.3],   # 样本1\n",
    "    [0.1, 2.1, 0.3],   # 样本2\n",
    "    [0.2, 0.4, 1.9]    # 样本3\n",
    "])\n",
    "\n",
    "# 对应的真实标签（类别编号，不是 one-hot）\n",
    "# 表示样本1是类别0，样本2是类别1，样本3是类别2\n",
    "targets = torch.tensor([0, 1, 2])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits, targets)\n",
    "\n",
    "print(\"平均交叉熵损失：\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7113, 0.1587, 0.1299],\n",
       "        [0.1041, 0.7689, 0.1271],\n",
       "        [0.1299, 0.1587, 0.7113]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(input=logits, dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
