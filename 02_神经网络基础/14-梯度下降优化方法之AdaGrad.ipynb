{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "> **AdaGrad è®©æ¯ä¸ªå‚æ•°éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„å­¦ä¹ ç‡ï¼Œå¹¶ä¸”è¿™ä¸ªå­¦ä¹ ç‡ä¼šéšè®­ç»ƒè¿‡ç¨‹åŠ¨æ€è°ƒæ•´ã€‚**\n",
    "\n",
    "å®ƒçš„ç›®æ ‡æ˜¯ï¼š\n",
    "- **è‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡**\n",
    "- **é€‚åº”ç¨€ç–æ•°æ®æˆ–ä¸å‡è¡¡ç‰¹å¾**\n",
    "- **é˜²æ­¢æŸäº›å‚æ•°æ›´æ–°å¤ªå¿«æˆ–å¤ªæ…¢**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” æ•°å­¦å…¬å¼\n",
    "\n",
    "è®¾ï¼š\n",
    "- $ \\theta $ï¼šæ¨¡å‹å‚æ•°\n",
    "- $ g_t $ï¼šå½“å‰æ¢¯åº¦\n",
    "- $ G_t $ï¼šå†å²æ¢¯åº¦å¹³æ–¹çš„ç´¯åŠ \n",
    "\n",
    "AdaGrad çš„æ›´æ–°å…¬å¼ä¸ºï¼š\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + g_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "- $ \\eta $ï¼šåˆå§‹å­¦ä¹ ç‡\n",
    "- $ \\epsilon $ï¼šé˜²æ­¢é™¤ä»¥ 0 çš„å°å¸¸æ•°ï¼ˆå¦‚ $10^{-8}$ï¼‰\n",
    "\n",
    "ğŸ‘‰ ç”±äº $ G_t $ ä¼šä¸æ–­å¢å¤§ï¼Œ**åˆ†æ¯å˜å¤§ â†’ å®é™…å­¦ä¹ ç‡å˜å°**ï¼Œæ‰€ä»¥ï¼š\n",
    "- æ¢¯åº¦è¾ƒå¤§çš„å‚æ•°æ›´æ–°å¹…åº¦å˜å°\n",
    "- æ¢¯åº¦è¾ƒå°çš„å‚æ•°å¯ä»¥æ›´æ–°å¾—æ›´å¤š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ AdaGrad çš„ä¼˜ç¼ºç‚¹\n",
    "\n",
    "| âœ… ä¼˜ç‚¹ | âŒ ç¼ºç‚¹ |\n",
    "|--------|--------|\n",
    "| è‡ªåŠ¨è°ƒèŠ‚å­¦ä¹ ç‡ | å­¦ä¹ ç‡ä¼šè¶Šæ¥è¶Šå°ï¼Œå¯èƒ½æå‰â€œåœæ­¢å­¦ä¹ â€ |\n",
    "| é€‚åˆç¨€ç–æ•°æ®ï¼ˆå¦‚ NLPï¼‰ | åœ¨éç¨€ç–ä»»åŠ¡ä¸­å®¹æ˜“ä¸‹é™å¤ªå¿«è€Œåœæ» |\n",
    "| æ˜“å®ç°ã€æ”¶æ•›ç¨³å®š | æ— æ³•â€œæ¢å¤â€å­¦ä¹ ç‡ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([0.9900], requires_grad=True) tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "tensor([0.9900]) tensor([0.9830], requires_grad=True) tensor([0.4901], grad_fn=<DivBackward0>)\n",
      "tensor([0.9830]) tensor([0.9772], requires_grad=True) tensor([0.4831], grad_fn=<DivBackward0>)\n",
      "tensor([0.9772]) tensor([0.9723], requires_grad=True) tensor([0.4775], grad_fn=<DivBackward0>)\n",
      "tensor([0.9723]) tensor([0.9679], requires_grad=True) tensor([0.4727], grad_fn=<DivBackward0>)\n",
      "tensor([0.9679]) tensor([0.9638], requires_grad=True) tensor([0.4684], grad_fn=<DivBackward0>)\n",
      "tensor([0.9638]) tensor([0.9601], requires_grad=True) tensor([0.4645], grad_fn=<DivBackward0>)\n",
      "tensor([0.9601]) tensor([0.9567], requires_grad=True) tensor([0.4609], grad_fn=<DivBackward0>)\n",
      "tensor([0.9567]) tensor([0.9534], requires_grad=True) tensor([0.4576], grad_fn=<DivBackward0>)\n",
      "tensor([0.9534]) tensor([0.9503], requires_grad=True) tensor([0.4545], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "w = torch.tensor(data=[1.0], requires_grad=True, dtype=torch.float32)\n",
    "optimizer = optim.Adagrad(params=[w], lr=0.01)\n",
    "\n",
    "for i in range(10):\n",
    "    loss = (w**2)/2.0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(w.grad, w, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
