{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 核心思想\n",
    "\n",
    "> **AdaGrad 让每个参数都有自己独立的学习率，并且这个学习率会随训练过程动态调整。**\n",
    "\n",
    "它的目标是：\n",
    "- **自动调节学习率**\n",
    "- **适应稀疏数据或不均衡特征**\n",
    "- **防止某些参数更新太快或太慢**\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 数学公式\n",
    "\n",
    "设：\n",
    "- $ \\theta $：模型参数\n",
    "- $ g_t $：当前梯度\n",
    "- $ G_t $：历史梯度平方的累加\n",
    "\n",
    "AdaGrad 的更新公式为：\n",
    "\n",
    "$$\n",
    "G_t = G_{t-1} + g_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{G_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ \\eta $：初始学习率\n",
    "- $ \\epsilon $：防止除以 0 的小常数（如 $10^{-8}$）\n",
    "\n",
    "👉 由于 $ G_t $ 会不断增大，**分母变大 → 实际学习率变小**，所以：\n",
    "- 梯度较大的参数更新幅度变小\n",
    "- 梯度较小的参数可以更新得更多\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 AdaGrad 的优缺点\n",
    "\n",
    "| ✅ 优点 | ❌ 缺点 |\n",
    "|--------|--------|\n",
    "| 自动调节学习率 | 学习率会越来越小，可能提前“停止学习” |\n",
    "| 适合稀疏数据（如 NLP） | 在非稀疏任务中容易下降太快而停滞 |\n",
    "| 易实现、收敛稳定 | 无法“恢复”学习率 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([0.9900], requires_grad=True) tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "tensor([0.9900]) tensor([0.9830], requires_grad=True) tensor([0.4901], grad_fn=<DivBackward0>)\n",
      "tensor([0.9830]) tensor([0.9772], requires_grad=True) tensor([0.4831], grad_fn=<DivBackward0>)\n",
      "tensor([0.9772]) tensor([0.9723], requires_grad=True) tensor([0.4775], grad_fn=<DivBackward0>)\n",
      "tensor([0.9723]) tensor([0.9679], requires_grad=True) tensor([0.4727], grad_fn=<DivBackward0>)\n",
      "tensor([0.9679]) tensor([0.9638], requires_grad=True) tensor([0.4684], grad_fn=<DivBackward0>)\n",
      "tensor([0.9638]) tensor([0.9601], requires_grad=True) tensor([0.4645], grad_fn=<DivBackward0>)\n",
      "tensor([0.9601]) tensor([0.9567], requires_grad=True) tensor([0.4609], grad_fn=<DivBackward0>)\n",
      "tensor([0.9567]) tensor([0.9534], requires_grad=True) tensor([0.4576], grad_fn=<DivBackward0>)\n",
      "tensor([0.9534]) tensor([0.9503], requires_grad=True) tensor([0.4545], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "w = torch.tensor(data=[1.0], requires_grad=True, dtype=torch.float32)\n",
    "optimizer = optim.Adagrad(params=[w], lr=0.01)\n",
    "\n",
    "for i in range(10):\n",
    "    loss = (w**2)/2.0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(w.grad, w, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
