{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 🧠 核心思想\n",
    "\n",
    "回顾一下 AdaGrad 的问题：\n",
    "\n",
    "- 它会把所有历史梯度平方累加，导致分母越来越大，**学习率越来越小，训练容易提前停滞**。\n",
    "\n",
    "RMSprop 的思路是：\n",
    "\n",
    "> **不用累加历史梯度的总和，而是对历史梯度做**指数加权平均（Exponential Moving Average）。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔍 公式\n",
    "\n",
    "设：\n",
    "- $ g_t $：当前梯度\n",
    "- $ E[g^2]_t $：梯度平方的滑动平均\n",
    "- $ \\gamma $：衰减率（常用值为 0.9）\n",
    "- $ \\eta $：学习率\n",
    "\n",
    "RMSprop 的更新规则：\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "> 相当于：**让学习率对过去梯度“有记忆但逐渐遗忘”，保持动态而不会过早变小。**\n",
    "\n",
    "---\n",
    "\n",
    "### 📈 RMSprop 的优缺点\n",
    "\n",
    "| ✅ 优点 | ❌ 缺点 |\n",
    "|--------|--------|\n",
    "| 适合非凸问题，收敛更稳 | 没有动量项（比 Adam 稍逊） |\n",
    "| 避免学习率过快衰减 | 仍需调参（lr, alpha） |\n",
    "| 表现比 AdaGrad 更强大 | 有时不如 Adam 一致稳定 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([0.9684], requires_grad=True) tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "tensor([0.9684]) tensor([0.9458], requires_grad=True) tensor([0.4689], grad_fn=<DivBackward0>)\n",
      "tensor([0.9458]) tensor([0.9271], requires_grad=True) tensor([0.4473], grad_fn=<DivBackward0>)\n",
      "tensor([0.9271]) tensor([0.9105], requires_grad=True) tensor([0.4297], grad_fn=<DivBackward0>)\n",
      "tensor([0.9105]) tensor([0.8955], requires_grad=True) tensor([0.4145], grad_fn=<DivBackward0>)\n",
      "tensor([0.8955]) tensor([0.8815], requires_grad=True) tensor([0.4010], grad_fn=<DivBackward0>)\n",
      "tensor([0.8815]) tensor([0.8683], requires_grad=True) tensor([0.3885], grad_fn=<DivBackward0>)\n",
      "tensor([0.8683]) tensor([0.8558], requires_grad=True) tensor([0.3770], grad_fn=<DivBackward0>)\n",
      "tensor([0.8558]) tensor([0.8437], requires_grad=True) tensor([0.3662], grad_fn=<DivBackward0>)\n",
      "tensor([0.8437]) tensor([0.8321], requires_grad=True) tensor([0.3559], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "w = torch.tensor(data=[1.0], requires_grad=True, dtype=torch.float32)\n",
    "optimizer = optim.RMSprop(params=[w], lr=0.01, alpha=0.9)\n",
    "\n",
    "for i in range(10):\n",
    "    loss = (w**2)/2.0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(w.grad, w, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
