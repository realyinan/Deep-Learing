{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ§  æ ¸å¿ƒæ€æƒ³\n",
    "\n",
    "å›é¡¾ä¸€ä¸‹ AdaGrad çš„é—®é¢˜ï¼š\n",
    "\n",
    "- å®ƒä¼šæŠŠæ‰€æœ‰å†å²æ¢¯åº¦å¹³æ–¹ç´¯åŠ ï¼Œå¯¼è‡´åˆ†æ¯è¶Šæ¥è¶Šå¤§ï¼Œ**å­¦ä¹ ç‡è¶Šæ¥è¶Šå°ï¼Œè®­ç»ƒå®¹æ˜“æå‰åœæ»**ã€‚\n",
    "\n",
    "RMSprop çš„æ€è·¯æ˜¯ï¼š\n",
    "\n",
    "> **ä¸ç”¨ç´¯åŠ å†å²æ¢¯åº¦çš„æ€»å’Œï¼Œè€Œæ˜¯å¯¹å†å²æ¢¯åº¦åš**æŒ‡æ•°åŠ æƒå¹³å‡ï¼ˆExponential Moving Averageï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ” å…¬å¼\n",
    "\n",
    "è®¾ï¼š\n",
    "- $ g_t $ï¼šå½“å‰æ¢¯åº¦\n",
    "- $ E[g^2]_t $ï¼šæ¢¯åº¦å¹³æ–¹çš„æ»‘åŠ¨å¹³å‡\n",
    "- $ \\gamma $ï¼šè¡°å‡ç‡ï¼ˆå¸¸ç”¨å€¼ä¸º 0.9ï¼‰\n",
    "- $ \\eta $ï¼šå­¦ä¹ ç‡\n",
    "\n",
    "RMSprop çš„æ›´æ–°è§„åˆ™ï¼š\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\gamma E[g^2]_{t-1} + (1 - \\gamma) g_t^2\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} \\cdot g_t\n",
    "$$\n",
    "\n",
    "> ç›¸å½“äºï¼š**è®©å­¦ä¹ ç‡å¯¹è¿‡å»æ¢¯åº¦â€œæœ‰è®°å¿†ä½†é€æ¸é—å¿˜â€ï¼Œä¿æŒåŠ¨æ€è€Œä¸ä¼šè¿‡æ—©å˜å°ã€‚**\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“ˆ RMSprop çš„ä¼˜ç¼ºç‚¹\n",
    "\n",
    "| âœ… ä¼˜ç‚¹ | âŒ ç¼ºç‚¹ |\n",
    "|--------|--------|\n",
    "| é€‚åˆéå‡¸é—®é¢˜ï¼Œæ”¶æ•›æ›´ç¨³ | æ²¡æœ‰åŠ¨é‡é¡¹ï¼ˆæ¯” Adam ç¨é€Šï¼‰ |\n",
    "| é¿å…å­¦ä¹ ç‡è¿‡å¿«è¡°å‡ | ä»éœ€è°ƒå‚ï¼ˆlr, alphaï¼‰ |\n",
    "| è¡¨ç°æ¯” AdaGrad æ›´å¼ºå¤§ | æœ‰æ—¶ä¸å¦‚ Adam ä¸€è‡´ç¨³å®š |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([0.9684], requires_grad=True) tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "tensor([0.9684]) tensor([0.9458], requires_grad=True) tensor([0.4689], grad_fn=<DivBackward0>)\n",
      "tensor([0.9458]) tensor([0.9271], requires_grad=True) tensor([0.4473], grad_fn=<DivBackward0>)\n",
      "tensor([0.9271]) tensor([0.9105], requires_grad=True) tensor([0.4297], grad_fn=<DivBackward0>)\n",
      "tensor([0.9105]) tensor([0.8955], requires_grad=True) tensor([0.4145], grad_fn=<DivBackward0>)\n",
      "tensor([0.8955]) tensor([0.8815], requires_grad=True) tensor([0.4010], grad_fn=<DivBackward0>)\n",
      "tensor([0.8815]) tensor([0.8683], requires_grad=True) tensor([0.3885], grad_fn=<DivBackward0>)\n",
      "tensor([0.8683]) tensor([0.8558], requires_grad=True) tensor([0.3770], grad_fn=<DivBackward0>)\n",
      "tensor([0.8558]) tensor([0.8437], requires_grad=True) tensor([0.3662], grad_fn=<DivBackward0>)\n",
      "tensor([0.8437]) tensor([0.8321], requires_grad=True) tensor([0.3559], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "w = torch.tensor(data=[1.0], requires_grad=True, dtype=torch.float32)\n",
    "optimizer = optim.RMSprop(params=[w], lr=0.01, alpha=0.9)\n",
    "\n",
    "for i in range(10):\n",
    "    loss = (w**2)/2.0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(w.grad, w, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
