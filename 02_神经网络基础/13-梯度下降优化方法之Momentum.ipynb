{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” å…¬å¼è§£é‡Š\n",
    "\n",
    "æ™®é€šçš„æ¢¯åº¦ä¸‹é™ï¼š\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\cdot \\nabla L(\\theta_t)\n",
    "$$\n",
    "\n",
    "åŠ å…¥åŠ¨é‡åçš„æ¢¯åº¦ä¸‹é™ï¼š\n",
    "\n",
    "$$\n",
    "v_{t+1} = \\gamma v_t + \\eta \\cdot \\nabla L(\\theta_t)\n",
    "$$\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - v_{t+1}\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ï¼š\n",
    "\n",
    "- $ \\theta $ï¼šæ¨¡å‹å‚æ•°  \n",
    "- $ \\eta $ï¼šå­¦ä¹ ç‡ï¼ˆlearning rateï¼‰  \n",
    "- $ \\gamma $ï¼šåŠ¨é‡ç³»æ•°ï¼ˆé€šå¸¸åœ¨ 0.9 å·¦å³ï¼‰  \n",
    "- $ v_t $ï¼šä¹‹å‰çš„æ¢¯åº¦ç´¯ç§¯ï¼ˆé€Ÿåº¦ï¼‰\n",
    "\n",
    "### ğŸ¯ æ€»ç»“\n",
    "\n",
    "| æ–¹æ³• | ç‰¹ç‚¹ |\n",
    "|------|------|\n",
    "| æ™®é€šæ¢¯åº¦ä¸‹é™ | æ›´æ–°æ–¹å‘ç”±å½“å‰æ¢¯åº¦å†³å®šï¼Œå®¹æ˜“éœ‡è¡ |\n",
    "| Momentum | åˆ©ç”¨è¿‡å»æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼ŒæŠ‘åˆ¶éœ‡è¡ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) tensor([0.9900], requires_grad=True) tensor([0.5000], grad_fn=<DivBackward0>)\n",
      "tensor([0.9900]) tensor([0.9711], requires_grad=True) tensor([0.4901], grad_fn=<DivBackward0>)\n",
      "tensor([0.9711]) tensor([0.9444], requires_grad=True) tensor([0.4715], grad_fn=<DivBackward0>)\n",
      "tensor([0.9444]) tensor([0.9109], requires_grad=True) tensor([0.4459], grad_fn=<DivBackward0>)\n",
      "tensor([0.9109]) tensor([0.8716], requires_grad=True) tensor([0.4149], grad_fn=<DivBackward0>)\n",
      "tensor([0.8716]) tensor([0.8276], requires_grad=True) tensor([0.3799], grad_fn=<DivBackward0>)\n",
      "tensor([0.8276]) tensor([0.7797], requires_grad=True) tensor([0.3425], grad_fn=<DivBackward0>)\n",
      "tensor([0.7797]) tensor([0.7288], requires_grad=True) tensor([0.3039], grad_fn=<DivBackward0>)\n",
      "tensor([0.7288]) tensor([0.6756], requires_grad=True) tensor([0.2655], grad_fn=<DivBackward0>)\n",
      "tensor([0.6756]) tensor([0.6211], requires_grad=True) tensor([0.2282], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "\n",
    "w = torch.tensor(data=[1.0], requires_grad=True, dtype=torch.float32)\n",
    "optimizer = optim.SGD(params=[w], lr=0.01, momentum=0.9)\n",
    "\n",
    "for i in range(10):\n",
    "    loss = (w**2)/2.0\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(w.grad, w, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
