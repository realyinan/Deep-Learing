{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **å‰å‘ä¼ æ’­ï¼ˆForward Propagationï¼‰**  \n",
    "> **åå‘ä¼ æ’­ï¼ˆBackward Propagation / Backpropï¼‰**\n",
    "\n",
    "ç®€å•ç†è§£å°±æ˜¯ï¼š\n",
    "- **å‰å‘ä¼ æ’­**ï¼šè®©è¾“å…¥æ•°æ®ä»å¤´åˆ°å°¾è·‘ä¸€éï¼Œå¾—åˆ°é¢„æµ‹ç»“æœï¼›\n",
    "- **åå‘ä¼ æ’­**ï¼šè®¡ç®—è¯¯å·®å¦‚ä½•å½±å“æ¯ä¸€å±‚çš„å‚æ•°ï¼Œä»è€Œæ›´æ–°å®ƒä»¬ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  å‰å‘ä¼ æ’­ï¼ˆForwardï¼‰\n",
    "\n",
    "- æŠŠè¾“å…¥ $ x $ ä¸€å±‚å±‚ä¼ ä¸‹å»ï¼Œç»è¿‡æ¯ä¸€å±‚çš„æƒé‡ã€æ¿€æ´»å‡½æ•°ç­‰ï¼Œæœ€ç»ˆå¾—åˆ°é¢„æµ‹å€¼ $ \\hat{y} $ã€‚\n",
    "- è¿™æ˜¯ä¸€ä¸ª**å‰è¿›çš„è¿‡ç¨‹**ã€‚\n",
    "\n",
    "### ä¸¾ä¾‹ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š\n",
    "è¾“å…¥ä¸€ä¸ªæ ·æœ¬ $ x $ï¼š\n",
    "\n",
    "$$\n",
    "z_1 = W_1 x + b_1 \\\\\n",
    "a_1 = \\text{ReLU}(z_1) \\\\\n",
    "z_2 = W_2 a_1 + b_2 \\\\\n",
    "\\hat{y} = \\text{Softmax}(z_2)\n",
    "$$\n",
    "\n",
    "ä½ æŠŠæ¯ä¸€å±‚çš„è¾“å‡ºéƒ½å­˜èµ·æ¥ï¼Œä¸ºåç»­åå‘ä¼ æ’­ç”¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” åå‘ä¼ æ’­ï¼ˆBackwardï¼‰\n",
    "\n",
    "- æˆ‘ä»¬è®¡ç®—é¢„æµ‹å€¼ $ \\hat{y} $ ä¸çœŸå®å€¼ $ y $ çš„**æŸå¤±** $ L(\\hat{y}, y) $ï¼Œæ¯”å¦‚äº¤å‰ç†µæˆ– MSEã€‚\n",
    "- ç„¶åæˆ‘ä»¬è¦è®¡ç®—è¿™ä¸ªæŸå¤±å¯¹æ‰€æœ‰å‚æ•°ï¼ˆæƒé‡ï¼‰çš„**æ¢¯åº¦**ã€‚\n",
    "- å†é€šè¿‡**é“¾å¼æ³•åˆ™**ï¼Œä¸€å±‚å±‚å‘åæ¨å¯¼æ¯ä¸€å±‚å¯¹æŸå¤±çš„å½±å“ï¼ˆä¹Ÿå°±æ˜¯æ¢¯åº¦ï¼‰ï¼š\n",
    "  \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2}, \\quad \\frac{\\partial L}{\\partial W_1}, \\ldots\n",
    "$$\n",
    "\n",
    "- æœ€åç”¨è¿™äº›æ¢¯åº¦æ›´æ–°å‚æ•°ï¼ˆé€šè¿‡ SGDã€Adam ç­‰ä¼˜åŒ–å™¨ï¼‰ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch ä¸­çš„æ‰§è¡Œæµç¨‹ï¼š\n",
    "\n",
    "```python\n",
    "# å‰å‘ä¼ æ’­\n",
    "output = model(input)                # å¾—åˆ°é¢„æµ‹å€¼\n",
    "loss = criterion(output, target)     # è®¡ç®—æŸå¤±å‡½æ•°\n",
    "\n",
    "# åå‘ä¼ æ’­\n",
    "loss.backward()                      # è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ¢¯åº¦\n",
    "\n",
    "# å‚æ•°æ›´æ–°\n",
    "optimizer.step()                     # ç”¨æ¢¯åº¦æ›´æ–°æƒé‡\n",
    "optimizer.zero_grad()                # æ¸…ç©ºä¸Šæ¬¡æ¢¯åº¦\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… æ€»ç»“ä¸€å¥è¯ï¼š\n",
    "\n",
    "> **å‰å‘ä¼ æ’­** æ˜¯â€œé¢„æµ‹â€ï¼Œ**åå‘ä¼ æ’­** æ˜¯â€œå­¦ä¹ â€ã€‚\n",
    "\n",
    "- å‰å‘ä¼ æ’­è®¡ç®—é¢„æµ‹ç»“æœï¼›\n",
    "- åå‘ä¼ æ’­æ‰¾å‡ºæ¨¡å‹å“ªé‡Œé”™äº†ï¼Œä»¥åŠæ€ä¹ˆæ”¹ï¼ˆç®—æ¢¯åº¦ï¼‰ï¼›\n",
    "- æœ€åä¸€æ­¥æ˜¯é€šè¿‡ä¼˜åŒ–å™¨å»æ”¹æ¨¡å‹çš„å‚æ•°ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad:  tensor(-4.)\n",
      "b.grad:  tensor(-2.)\n",
      "é¢„æµ‹å€¼: 2.00, æŸå¤±: 2.00\n",
      "æ›´æ–°åå‚æ•°: w=1.40, b=0.20\n",
      "w.grad:  tensor(-2.)\n",
      "b.grad:  tensor(-1.)\n",
      "é¢„æµ‹å€¼: 3.00, æŸå¤±: 0.50\n",
      "æ›´æ–°åå‚æ•°: w=1.60, b=0.30\n",
      "w.grad:  tensor(-1.)\n",
      "b.grad:  tensor(-0.5000)\n",
      "é¢„æµ‹å€¼: 3.50, æŸå¤±: 0.12\n",
      "æ›´æ–°åå‚æ•°: w=1.70, b=0.35\n",
      "w.grad:  tensor(-0.5000)\n",
      "b.grad:  tensor(-0.2500)\n",
      "é¢„æµ‹å€¼: 3.75, æŸå¤±: 0.03\n",
      "æ›´æ–°åå‚æ•°: w=1.75, b=0.38\n",
      "w.grad:  tensor(-0.2500)\n",
      "b.grad:  tensor(-0.1250)\n",
      "é¢„æµ‹å€¼: 3.88, æŸå¤±: 0.01\n",
      "æ›´æ–°åå‚æ•°: w=1.77, b=0.39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. åˆå§‹åŒ–å‚æ•°ï¼ˆä½¿ç”¨ requires_grad è¿½è¸ªæ¢¯åº¦ï¼‰\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# 2. è¾“å…¥å’Œç›®æ ‡\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(4.0)\n",
    "\n",
    "# 3. å­¦ä¹ ç‡\n",
    "lr = 0.1\n",
    "\n",
    "# 4. ä¼˜åŒ–å™¨\n",
    "optimizer = torch.optim.SGD([w, b], lr=lr)\n",
    "\n",
    "# 5. è®­ç»ƒä¸€æ¬¡ï¼ˆ1æ¬¡å‰å‘ + åå‘ + æ›´æ–°ï¼‰\n",
    "for epoch in range(5):\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    y_pred = w * x + b\n",
    "    loss = 0.5 * (y_pred - y)**2  # MSEæŸå¤±ï¼ˆé™¤ä»¥2æ˜¯ä¸ºäº†åç»­å¯¼æ•°ç®€æ´ï¼‰\n",
    "\n",
    "    # æ¸…é›¶æ¢¯åº¦\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # åå‘ä¼ æ’­\n",
    "    loss.backward()\n",
    "    print(\"w.grad: \", w.grad)\n",
    "    print(\"b.grad: \", b.grad)\n",
    "\n",
    "    # å‚æ•°æ›´æ–°\n",
    "    optimizer.step()\n",
    "\n",
    "    # æ‰“å°\n",
    "    print(f\"é¢„æµ‹å€¼: {y_pred.item():.2f}, æŸå¤±: {loss.item():.2f}\")\n",
    "    print(f\"æ›´æ–°åå‚æ•°: w={w.item():.2f}, b={b.item():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
