{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **前向传播（Forward Propagation）**  \n",
    "> **反向传播（Backward Propagation / Backprop）**\n",
    "\n",
    "简单理解就是：\n",
    "- **前向传播**：让输入数据从头到尾跑一遍，得到预测结果；\n",
    "- **反向传播**：计算误差如何影响每一层的参数，从而更新它们。\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 前向传播（Forward）\n",
    "\n",
    "- 把输入 $ x $ 一层层传下去，经过每一层的权重、激活函数等，最终得到预测值 $ \\hat{y} $。\n",
    "- 这是一个**前进的过程**。\n",
    "\n",
    "### 举例（简化版）：\n",
    "输入一个样本 $ x $：\n",
    "\n",
    "$$\n",
    "z_1 = W_1 x + b_1 \\\\\n",
    "a_1 = \\text{ReLU}(z_1) \\\\\n",
    "z_2 = W_2 a_1 + b_2 \\\\\n",
    "\\hat{y} = \\text{Softmax}(z_2)\n",
    "$$\n",
    "\n",
    "你把每一层的输出都存起来，为后续反向传播用。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 反向传播（Backward）\n",
    "\n",
    "- 我们计算预测值 $ \\hat{y} $ 与真实值 $ y $ 的**损失** $ L(\\hat{y}, y) $，比如交叉熵或 MSE。\n",
    "- 然后我们要计算这个损失对所有参数（权重）的**梯度**。\n",
    "- 再通过**链式法则**，一层层向后推导每一层对损失的影响（也就是梯度）：\n",
    "  \n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_2}, \\quad \\frac{\\partial L}{\\partial W_1}, \\ldots\n",
    "$$\n",
    "\n",
    "- 最后用这些梯度更新参数（通过 SGD、Adam 等优化器）。\n",
    "\n",
    "---\n",
    "\n",
    "### PyTorch 中的执行流程：\n",
    "\n",
    "```python\n",
    "# 前向传播\n",
    "output = model(input)                # 得到预测值\n",
    "loss = criterion(output, target)     # 计算损失函数\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()                      # 自动计算所有梯度\n",
    "\n",
    "# 参数更新\n",
    "optimizer.step()                     # 用梯度更新权重\n",
    "optimizer.zero_grad()                # 清空上次梯度\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ 总结一句话：\n",
    "\n",
    "> **前向传播** 是“预测”，**反向传播** 是“学习”。\n",
    "\n",
    "- 前向传播计算预测结果；\n",
    "- 反向传播找出模型哪里错了，以及怎么改（算梯度）；\n",
    "- 最后一步是通过优化器去改模型的参数。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad:  tensor(-4.)\n",
      "b.grad:  tensor(-2.)\n",
      "预测值: 2.00, 损失: 2.00\n",
      "更新后参数: w=1.40, b=0.20\n",
      "w.grad:  tensor(-2.)\n",
      "b.grad:  tensor(-1.)\n",
      "预测值: 3.00, 损失: 0.50\n",
      "更新后参数: w=1.60, b=0.30\n",
      "w.grad:  tensor(-1.)\n",
      "b.grad:  tensor(-0.5000)\n",
      "预测值: 3.50, 损失: 0.12\n",
      "更新后参数: w=1.70, b=0.35\n",
      "w.grad:  tensor(-0.5000)\n",
      "b.grad:  tensor(-0.2500)\n",
      "预测值: 3.75, 损失: 0.03\n",
      "更新后参数: w=1.75, b=0.38\n",
      "w.grad:  tensor(-0.2500)\n",
      "b.grad:  tensor(-0.1250)\n",
      "预测值: 3.88, 损失: 0.01\n",
      "更新后参数: w=1.77, b=0.39\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. 初始化参数（使用 requires_grad 追踪梯度）\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# 2. 输入和目标\n",
    "x = torch.tensor(2.0)\n",
    "y = torch.tensor(4.0)\n",
    "\n",
    "# 3. 学习率\n",
    "lr = 0.1\n",
    "\n",
    "# 4. 优化器\n",
    "optimizer = torch.optim.SGD([w, b], lr=lr)\n",
    "\n",
    "# 5. 训练一次（1次前向 + 反向 + 更新）\n",
    "for epoch in range(5):\n",
    "    # 前向传播\n",
    "    y_pred = w * x + b\n",
    "    loss = 0.5 * (y_pred - y)**2  # MSE损失（除以2是为了后续导数简洁）\n",
    "\n",
    "    # 清零梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    print(\"w.grad: \", w.grad)\n",
    "    print(\"b.grad: \", b.grad)\n",
    "\n",
    "    # 参数更新\n",
    "    optimizer.step()\n",
    "\n",
    "    # 打印\n",
    "    print(f\"预测值: {y_pred.item():.2f}, 损失: {loss.item():.2f}\")\n",
    "    print(f\"更新后参数: w={w.item():.2f}, b={b.item():.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
